{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝(LSTM)을 사용하여 오늘의 명언 만들기\n",
    "\n",
    "* * *\n",
    "## 1. RNN과 LSTM\n",
    "### 1) RNN\n",
    "인공지능을 학습시키기 위한 신경망(Neural Network)에는 CNN, RNN, RBM과 DBN 등 여러가지 종류가 있습니다.\n",
    "\n",
    "그 중에서도 __RNN__은 Recurrent Neural Network 즉, 순환신경망의 약자로 순차적인 데이터를 다루는데 적합합니다.\n",
    "\n",
    "인간은 생각을 할 때 아무것도 모르는채 시작하지 않습니다. 즉, 배경지식을 가지고 생각합니다.\n",
    "전통적인 신경망은 인간과 같이 __'배경지식을 활용한 생각'을 하지 못한다는 것이 단점이였습니다.__\n",
    "이 문제를 해결하는 신경망이 바로 __RNN__입니다. \n",
    "\n",
    "RNN은 은닉층의 결과가 다시 같은 은닉층의 입력으로 들어가도록 되어있기 때문에 순서 또는 시간이라는 측면을 고려할 수 있으며, sequence data(시계열 데이터)를 처리하는데 효과적입니다. \n",
    "\n",
    "하지만 이런 RNN도 __장기 의존성(Long-Term Dependency)__이라는 문제점이 있습니다. 제공된 데이터와 참고해야할 정보의 입력 위치 차이(Gap) 커지면 RNN은 두 정보의 문맥을 연결하지 못하게 됩니다.\n",
    "\n",
    "이를 개선하기 위해, __LSTM(Long Short Term Memory)__가 개발되었습니다. RNN문제는 LSTM으로 어느정도 해결되어 자동 작곡, 작사, 저술, 주가 예측 등 다양한 분야에 활용되고 있습니다.\n",
    "\n",
    "\n",
    "### 2) LSTM\n",
    "LSTM은 Long Short-Term Memory의 약자로, __장단기 메모리__라고 합니다. \n",
    "Hochreiter & Schmidhuber(1997)에 의해 소개되었습니다. \n",
    "\n",
    "LSTM은 은닉층의 메모리 셀에 __입력 게이트, 삭제 게이트, 출력 게이트__를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정합니다. LSTM은 은닉 상태(hidden state)를 계산하는 식이 전통적인 RNN보다 조금 더 복잡해졌으며 __셀 상태(cell state)__라는 값을 추가하였습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) RNN과 LSTM의 구조비교\n",
    "모든 RNN은 신경망 모듈을 반복시키는 체인과 같은 형태로 되어져 있습니다. \n",
    "\n",
    "__기본적인 RNN 구조__는 아래와 같이 단순합니다.\n",
    "\n",
    "![title](./pictures/RNN_structure.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LSTM__도 기본적인 RNN 구조와 같이 체인과 같은 형태를 가지고 있지만, __각 반복 모듈은 다른 구조__를 갖고 있습니다.\n",
    "단순한 neural network layer 한 층 대신에 __4개의 layer가 특별한 방식으로 서로 정보를 주고 받도록__ 되어 있습니다.\n",
    "\n",
    "![title](./pictures/LSTM_structure.png) \n",
    "\n",
    "저희 조는 이와 같은 LSTM 신경망을 이용해 명언들을 학습시키고, 오늘의 명언을 만들어내는 프로젝트를 진행하게 되었습니다.\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 개발 환경 (for Windows)\n",
    "### 1) 파이썬, 아나콘다, 주피터 노트북\n",
    "위의 세가지는 기본적으로 설치 되어있어야 합니다. \n",
    "\n",
    "### 2) 주피터 노트북에 keras설치\n",
    "\n",
    "a. 먼저 Anaconda prompt(Anaconda 3)를 관리자 권한으로 실행합니다.\n",
    "\n",
    "b. 파이썬 패키지 소프트웨어를 설치, 관리하는 프로그램인 __pip__을 업그레이드 시켜줍니다\n",
    "    <pre><code>python -m pip install –upgrade pip</code></pre>\n",
    "\n",
    "c. keras라는 이름의 가상환경을 만듭니다. 가상환경이란 독립된 환경으로, 가상환경에서 설치된 라이브러리는 다른 환경에 영향을 주지 않습니다. 가상환경의 이름은 자신이 원하는 것으로 해주어도 괜찮습니다.  \n",
    "    <pre><code>conda create -n keras python=3.7</code></pre>\n",
    "\n",
    "d. 가상환경을 활성화시켜줍니다. \n",
    "    <pre><code>conda activate keras</code></pre>\n",
    "\n",
    "e. 가상환경용 주피터 노트북을 설치합니다. \n",
    "    <pre><code>conda install nb_conda</code></pre>\n",
    "    \n",
    "f. keras를 설치합니다.\n",
    "    <pre><code> pip install keras </code></pre>\n",
    "\n",
    "### 3) 가상환경에서 주피터 노트북 실행 \n",
    "a. 작업의 편리함을 위해 원하는 위치에 작업용 폴더를 하나 만듭니다.  \n",
    "\n",
    "b. git hub에서 saying.txt를 다운 받아 작업용 폴더에 넣어줍니다. \n",
    "\n",
    "c. prompt 창에서 명령어 cd를 이용해 작업용 폴더에 들어갑니다. \n",
    "\n",
    "d. 주피터 노트북을 실행시킵니다. \n",
    "    <pre><code> jupyter notebook </code></pre>\n",
    "    \n",
    "### 4) 프로그램 실행\n",
    "실행된 주피터 노트북에서 파이썬 notebook을 하나 만든 후, 아래의 코드를 복사/ 붙여 넣습니다.\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 코드 설명\n",
    "\n",
    "### 1) 모듈 가져오기\n",
    "딥러닝 프레임워크인 __keras__와 행렬 연산 패키지인 __numpy__ 모듈을 가져옵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 데이터 가공\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 26304\n"
     ]
    }
   ],
   "source": [
    "#file을 읽어옵니다\n",
    "path = 'saying.txt'\n",
    "with io.open(path, encoding='ansi') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "#정규표현식을 활용하여 쓸데없는 데이터를 가공합니다\n",
    "text = re.sub(r'<.*>', '', text) #<괄호>안에 있는 것들을 space로\n",
    "text = re.sub(r'\\n', ' ', text) #줄바꿈을 space로\n",
    "text = re.sub(r' +', ' ', text) #여러개의 space( )를 하나의 space로\n",
    "\n",
    "#text의 길이를 출력합니다 \n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Dictinoary 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 779\n"
     ]
    }
   ],
   "source": [
    "#한글자씩 때어 sort 시키고 chars에 저장 후 그길이를 출력합니다\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "\n",
    "# dict함수를 이용해 Key와 Value의 쌍을 튜플로 묶어 \n",
    "# dictonery를 만들어 준다\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 전처리\n",
    "우리는 10글자를 주었을 때 그 다음에 올 1글자를 예측하는 방식으로 인공지능을 학습시킬 것입니다. \n",
    "\n",
    "따라서 먼저 for문을 돌면서 sentences에는 10글자씩, next_chars에는 1글자씩 넣어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 8765\n"
     ]
    }
   ],
   "source": [
    "maxlen = 10\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "#sentences에는 10글자씩, next_chars에는 1글자씩 삽입 후 \n",
    "#sentences의 갯수가 몇개인지 출력\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen]) #input \n",
    "    next_chars.append(text[i + maxlen]) #output\n",
    "print('nb sequences:', len(sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) one hot incoding으로 char을 벡터화하기\n",
    "만약 chars에 하,나,님,을,따,르,는,사,람,들 이 저장되어있다면,\n",
    "이를 각각의 0과 1로 이루어진 고유한 배열로 만들어줍니다. \n",
    "\n",
    "예를 들면, 아래와 같이 unique하게 번호를 부여합니다.\n",
    "\n",
    "\n",
    "하 0(index)  = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n",
    "\n",
    "나 1  = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] \n",
    "\n",
    "님 2  = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] \n",
    "\n",
    "을 3 = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] \n",
    "\n",
    "(이하 생략)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "#one hot incoding으로 벡터화\n",
    "print('Vectorization...') \n",
    "\n",
    "#x=input, y=output\n",
    "#0으로 채워진 numpy 배열을 만든다\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "#sentences를 돌면서 해당하는 글자의 index에만 1을 넣어준다\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1 #input\n",
    "    y[i, char_indices[next_chars[i]]] = 1 #output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) 모델 생성 \n",
    "모델은 add하는 순서대로 만들어지게 됩니다. LSTM layer와 Dense layer를 만듭니다. \n",
    "\n",
    "__Dense layer__란, 입출력을 모두 연결해주는 layer로, 입력 뉴런 수에 상관없이 출력 뉴런 수를 자유롭게 설정할 수 있기 때문에 출력층으로 많이 사용됩니다. \n",
    "\n",
    "모델을 만든 후 compile()를 호출하여 모델이 효과적으로 구현될 수 있게 학습 과정에 필요한 loss(오차 함수)와 optimizer등의 환경을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential() \n",
    "\n",
    "#LSTM layer와 Dense layer를 만듭니다\n",
    "model.add(LSTM(1024, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "#loss(오차 함수)와 optimizer등의 환경을 설정하며 compile 합니다\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) call-back 직접 구현 \n",
    "\n",
    "__call-back 함수__란, 내가 호출할 함수가 아니라 다른 것에 의해 호출 당할 함수를 말합니다.\n",
    "\n",
    "이 프로그램에서는 on_epoch_end()함수가 sample()이라는 함수를 호출하기 때문에 sample()을 call-back 함수라고 합니다. \n",
    "\n",
    "keras에서 기본적으로 지원하는 call-back 함수가 있지만, 우리가 진행하는 프로젝트에서 이를 사용하면 data에 따라 반복되는 단어와 문장에 의해 loop에 빠질 수 있습니다. \n",
    "\n",
    "이를 방지하기 위해 우리가 직접 random을(돌발성) 부여하여 call-back 함수를 정의하여 사용해줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8)  on_epoch_end 함수 구현\n",
    "한 epoch가 끝날때마다 명언을 생성하는 함수를 만들어줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한 epoch이 끝날때 마다 실행됩니다\n",
    "def on_epoch_end(epoch, _):\n",
    "    print('\\n----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    #랜덤으로 글자를 뽑을 위치를 설정\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "#     for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "#         print('----- diversity:', diversity)\n",
    "\n",
    "    #start_index부터 글자 10개를 뽑아서 sentense에 저장\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    #sentense에 저장된 글자 10개를 one hot incoding으로 벡터화\n",
    "    for i in range(10):\n",
    "        #0으로 초기화된 'x_pred'라는 이름의 numpy 배열을 생성\n",
    "        x_pred = np.zeros((1, maxlen, len(chars))) \n",
    "        \n",
    "        #sentense안의 글자에 해당하는 index를 1로 설정해줌\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        #'x_pred'를 input으로 받아 fit함수로 학습시켰던 모델을 가지고 다음 글자 예측\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "        # 그 글자의 index가 무엇인지 받아오고 index를 이용해 char을 받아옴\n",
    "        next_index = sample(preds, 0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        #받아온 한글자씩 generated라는 list에 넣음\n",
    "        generated += next_char\n",
    "        \n",
    "        #한글자를 처리 했으므로 sentense를 한칸씩 움직여 그 다음 글자부터 시작해서 이번에 생성한 char까지로 변경\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char) #출력\n",
    "        sys.stdout.flush() #입력버퍼 비우기\n",
    "    print()\n",
    "\n",
    "#on_epoch_end에서 사용될 call-back을 등록, print_callback이라는 변수에 저장\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "8765/8765 [==============================] - 54s 6ms/step - loss: 5.9779\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"살아갈 수 있다. \"\n",
      "살아갈 수 있다. 하이    을   \n",
      "Epoch 2/60\n",
      "8765/8765 [==============================] - 59s 7ms/step - loss: 4.5291\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"는 것은 책임감(r\"\n",
      "는 것은 책임감(r한 된이 이    \n",
      "Epoch 3/60\n",
      "8765/8765 [==============================] - 56s 6ms/step - loss: 4.5027\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"당신이 리더라고 생\"\n",
      "당신이 리더라고 생이         \n",
      "Epoch 4/60\n",
      "8765/8765 [==============================] - 52s 6ms/step - loss: 4.4527\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \" 나갔을 때 떳떳이\"\n",
      " 나갔을 때 떳떳이것그쳐이이어다는 을\n",
      "Epoch 5/60\n",
      "8765/8765 [==============================] - 58s 7ms/step - loss: 4.4416\n",
      "\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"기기는 쉬워도 이긴\"\n",
      "기기는 쉬워도 이긴다이이지 이은  이\n",
      "Epoch 6/60\n",
      "8765/8765 [==============================] - 58s 7ms/step - loss: 4.3800\n",
      "\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"되이 보낸 오늘은 \"\n",
      "되이 보낸 오늘은      이 는  \n",
      "Epoch 7/60\n",
      "8765/8765 [==============================] - 58s 7ms/step - loss: 4.3317\n",
      "\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"력으로 정복해야 한\"\n",
      "력으로 정복해야 한     .    \n",
      "Epoch 8/60\n",
      "8765/8765 [==============================] - 64s 7ms/step - loss: 4.1830\n",
      "\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"가가 사람을 위해 \"\n",
      "가가 사람을 위해   이  하이  이\n",
      "Epoch 9/60\n",
      "5120/8765 [================>.............] - ETA: 24s - loss: 4.1028"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-33fdf0eba7f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#학습시작: x와 y를 넣고 batch_size와 몇번 반복할지(epoch)를 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#default: batch_size=128, epoch=20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprint_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#학습시작: x와 y를 넣고 batch_size와 몇번 반복할지(epoch)를 설정\n",
    "#default: batch_size=128, epoch=20\n",
    "model.fit(x, y, batch_size=1024, epochs=60, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시간이 오래 걸리기 때문에 아래에 한 실행결과를 첨부했다.\n",
    "\n",
    "<처음>\n",
    "![title](./pictures/first_result.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<마지막>\n",
    "![title](./pictures/last_result.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 결과 분석\n",
    "\n",
    "데이터가 부족하여 결과가 잘나오지 않았다.\n",
    "\n",
    "또한 한글의 특성 상 한 글자를 처리할 때 초성 중성 종성을 분리하여 하면 더 좋은 결과가 나올 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
